{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially loading the Dataset and pre-req operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter\n",
    "from transformers import pipeline\n",
    "nltk.download('punkt_tab')\n",
    "# Load dataset\n",
    "file_path = \"/content/Cleaned Smart Email Dataset.csv\"\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarization Methods and Operations Involved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# The function for extractive summarization using TF-IDF and cosine similarity\n",
    "def extractive_summary(text, num_sentences=2):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    if len(sentences) <= num_sentences:\n",
    "        return text  # Return full text if it's already short\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    sentence_vectors = vectorizer.fit_transform(sentences)\n",
    "    similarity_matrix = cosine_similarity(sentence_vectors)\n",
    "    ranked_sentences = sorted(((similarity_matrix[i].sum(), s) for i, s in enumerate(sentences)), reverse=True)\n",
    "    summary = \" \".join([s for _, s in ranked_sentences[:num_sentences]])\n",
    "    return summary\n",
    "\n",
    "# We load transformer model for abstractive summarization\n",
    "summarizer = pipeline(\"summarization\", model=\"t5-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abstractive Summarization Implementation (Analyse and frame a new text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Function for abstractive summarization\n",
    "def abstractive_summary(text):\n",
    "    if len(text.split()) < 50:\n",
    "        return text  # Avoid summarization for short text\n",
    "    return summarizer(text, max_length=50, min_length=20, do_sample=False)[0]['summary_text']\n",
    "# Apply summarization to dataset\n",
    "df['Extractive_Summary'] = df['Email Content'].apply(lambda x: extractive_summary(str(x)))\n",
    "df['Abstractive_Summary'] = df['Email Content'].apply(lambda x: abstractive_summary(str(x)))\n",
    "# We have to Save the summarized dataset\n",
    "df.to_csv(\"/mnt/data/Summarized_Smart_Email_Dataset.csv\", index=False)\n",
    "\n",
    "print(\"Summarization completed. Extractive and Abstractive summaries added.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweeking the previous implementation for summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"/content/Cleaned Smart Email Dataset.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Function for extractive summarization using TF-IDF and cosine similarity\n",
    "def extractive_summary(text, num_sentences=2):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    if len(sentences) <= num_sentences:\n",
    "        return text  # Return full text if it's already short\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    sentence_vectors = vectorizer.fit_transform(sentences)\n",
    "    similarity_matrix = cosine_similarity(sentence_vectors)\n",
    "\n",
    "    ranked_sentences = sorted(((similarity_matrix[i].sum(), s) for i, s in enumerate(sentences)), reverse=True)\n",
    "    summary = \" \".join([s for _, s in ranked_sentences[:num_sentences]])\n",
    "    return summary\n",
    "# Load transformer model for abstractive summarization\n",
    "summarizer = pipeline(\"summarization\", model=\"t5-small\", device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "# Function for abstractive summarization with proper token chunking\n",
    "def abstractive_summary(text, tokenizer_max_length=512):\n",
    "    words = text.split()\n",
    "    if len(words) < 50:\n",
    "        return text  # Avoid summarization for short text\n",
    "   # Ensure chunks fit within token limit\n",
    "    max_tokens = tokenizer_max_length - 10  # Leave margin for safety\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    for word in words:\n",
    "        current_length += len(word) + 1  # Account for spaces\n",
    "        if current_length > max_tokens:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = [word]\n",
    "            current_length = len(word) + 1\n",
    "        else:\n",
    "            current_chunk.append(word)\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "# Summarize each chunk separately"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
